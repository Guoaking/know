##


## 镜像
传统考虑依赖,
镜像本质是一个压缩文件, 关键点事layer层,每层定义了一套文件和目录结构, 各层之间是独立的, 使用UNionFs联合在一起, 方便的分层传输, 复用以及基于已有layer的镜像定制
docker image inspect GraphDriver
RootFs 几层
容器是镜像运行的实例, 本质上是一种特殊的进程, 会指定一组namespace, 只能使用指定的配置, cgroup 做资源限制,

master节点
    apiserver, 对外提供api, https, 6443 数据存储在etcd, 无状态 节点状态对等
    scheduler, , listwatch机制, 类似长链接, 节点调度, 待处理的pod, node 有主从, 默认2s一次看锁有没有释放 一致性: 先更新缓存, 内存内保障一致性
        * predicate-预选策略, 拉node信息, 根据pod信息, 调度要求选出符合要求的pod,
        * priority 优选策略, 对node打分, 0-10 weight 没有顺序
    ControllerManage 内部资源的控制器 分多个controller endpoint anntion 节点在做处理

node节点 实际运行, docker,
* kubelet 维护本节点上的pod状态, 根据从apiserver监听的状态, 对对应的pod做变更
  * 节点信息上报, heapster
  * 镜像管理,
  * 容器健康检查 readiness, livenes
  * volume管理

## pod
* 特殊的进程组 一个pod 1-n个进程, 单pod所有容器共享一个networkNamespace 日志不要写进容器
* Qos Class : 资源保障


## deployment

> 无状态, 滚动更新, 水平扩缩容 spec-> strategy

## sls

* 存储状态
* 拓扑状态


etcd -> apiserver
cm 逻辑控制中心   监听到镜像更新-> watch -> 看实例镜像是否符合预期,  很多mangeer
scheduler  -> 把工作负载放在合适的节点
kubelet -> cri docker | cd , cni 容器网络接口, csi, 存储接口, 挂盘场景  crd
kubeprxy  负载均衡 代理pod的流量去哪里 虚拟ip转实际ip


overlay2 -> service 代理

5000个节点可以通, ingress 边界路由器 -> 转发7层流量 -> 代理到service -> service -> 服务

小于5000个节点
systemd去管理
容器网络




1. eeconf 为啥要下载到saltmaster
2. iaas deploycli学习
3. salt 模块
4. linux top等命令  top hop strace perf free iostat
5. 业务中使用的正向代理, 反向代理
6. 业务中网络怎么出去, 怎么进来, 四层七层
7. rds部署?
8. 业务告警, 物理机告警实现?
9. dockerfile 文件编写
   1.  https://yeasy.gitbook.io/docker_practice/image
10. docker cgroup namespace
    1.  https://yeasy.gitbook.io/docker_practice/security
11. k8s 调度几个状态, 为啥pending
    1.  pod 阶段status.phase: Pending, Running, Succeeded, Failed, Unknown
    2.  已经分配给某个节点, 创建容器时:容器状态: Waiting(ImagePullBackOff,CrashLoopBackoff,Error,CreateContainerError), Running, Terminated, kde(status)
    3.  ?
12. k8s 创建一个pod 流程
13. k8s 分组统计输出
    1.  kubectl get pods -A -ojsonpath='{range .items[*]}{.status.phase}{"\n"}' | sort |uniq -c |sort -k1nr
    2.  kubectl get pod deployment-toutiao-videoarch-tinyimg-57f68c869f-kvq44 -o go-template='{{range .status.conditions}}{{if eq .type "PodScheduled"}}{{.status}}{{end}}{{end}}{{"\n"}}'
14. 遇到有难点的问题, 以及解决
15. 标品目前存在的问题, 建议看法


ImagePullBackOff: pkg/kubelet/images/types.go
CreateContainerError: pkg/kubelet/kuberuntime/kuberuntime_container.go
ContainerCreating: pkg/kubelet/kubelet_pods.go
RunContainerError: pkg/kubelet/container/sync_result.go
ContainersNotInitialized: pkg/kubelet/status/generate.go
// 镜像error
// 容器error
//
pending : https://cloud.tencent.com/document/product/457/42948
1. 未被调度到节点上, 资源不够
2. 不满足Selector与affinity
3. node存在pod没有容忍的误点
4. kube-scheduler未正常运行
5. 驱逐后其他可用节点与当前节点的状态应用不在相同可用区




deployment好resplicaset的关系?  Owner References

Infomer 模式:
控制器模式:
责任链模式:


暂停容器充当 pod 中所有容器的“父容器
1. 它是 Pod 中 Linux 命名空间共享的基础。
2. 在启用 PID（进程 ID）命名空间共享的情况下，它充当每个 pod 的 PID 1 并收获僵尸进程。









