##


## 镜像
传统考虑依赖,
镜像本质是一个压缩文件, 关键点事layer层,每层定义了一套文件和目录结构, 各层之间是独立的, 使用UNionFs联合在一起, 方便的分层传输, 复用以及基于已有layer的镜像定制
docker image inspect GraphDriver
RootFs 几层
容器是镜像运行的实例, 本质上是一种特殊的进程, 会指定一组namespace, 只能使用指定的配置, cgroup 做资源限制,

master节点
    apiserver, 对外提供api, https, 6443 数据存储在etcd, 无状态 节点状态对等
    scheduler, , listwatch机制, 类似长链接, 节点调度, 待处理的pod, node 有主从, 默认2s一次看锁有没有释放 一致性: 先更新缓存, 内存内保障一致性
        * predicate-预选策略, 拉node信息, 根据pod信息, 调度要求选出符合要求的pod,
        * priority 优选策略, 对node打分, 0-10 weight 没有顺序
    ControllerManage 内部资源的控制器 分多个controller endpoint anntion 节点在做处理

node节点 实际运行, docker,
* kubelet 维护本节点上的pod状态, 根据从apiserver监听的状态, 对对应的pod做变更
  * 节点信息上报, heapster
  * 镜像管理,
  * 容器健康检查 readiness, livenes
  * volume管理

## pod
* 特殊的进程组 一个pod 1-n个进程, 单pod所有容器共享一个networkNamespace 日志不要写进容器
* Qos Class : 资源保障


## deployment

> 无状态, 滚动更新, 水平扩缩容 spec-> strategy

## sls

* 存储状态
* 拓扑状态


etcd -> apiserver
cm 逻辑控制中心   监听到镜像更新-> watch -> 看实例镜像是否符合预期,  很多mangeer
scheduler  -> 把工作负载放在合适的节点
kubelet -> cri docker | cd , cni 容器网络接口, csi, 存储接口, 挂盘场景  crd
kubeprxy  负载均衡 代理pod的流量去哪里 虚拟ip转实际ip


overlay2 -> service 代理

5000个节点可以通, ingress 边界路由器 -> 转发7层流量 -> 代理到service -> service -> 服务

小于5000个节点
systemd去管理
容器网络